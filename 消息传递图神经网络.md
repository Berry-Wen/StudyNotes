# 消息传递（Message Passing）范式
非结构化数据若要使用计算机进行处理，一般要将其转换为结构化数据，例如文本数据的 Word embedding 技术。类似地，对于由节点和边组成的图，特别是执行节点分类等任务，节点的表征（Node Representation，也称 Node Embedding）是成功的关键。  
Word embedding 的主要思想是利用目标词汇的上下文信息来理解该词汇的语义。类似地，节点表征也可考虑邻接节点的信息进行生成。这就是消息传递范式，它将卷积算子推广到了不规则数据领域，实现了图与神经网络的连接。遵循消息传递范式的图神经网络被称为消息传递图神经网络。  
为了降低直接理解消息传递范式的困难，可以先了解下有类似思想的标签传播算法。
## 标签传播算法（Label Propagation Algorithm, LPA）
标签传播算法是基于图的半监督学习方法，基本思路是从已标记的节点的标签信息来预测未标记的节点的标签信息。节点会选择自己邻居中出现次数最多的标签，如果每个标签出现次数一样多，那么就随机选择一个标签替换自己原始的标签，如此往复，直到每个节点标签不再发生变化，那么持有相同标签的节点就归为一个社区。  
* 优点：思路简单，时间复杂度低，适合大型复杂网络。  
* 缺点：划分结果不稳定（节点标签更新顺序随机），随机性强（随机选择）是致命的缺点。  
算法步骤：  
1. 先给每个节点分配对应标签，有的话直接用，没有就随机生成；  
2. 遍历N个节点（for i=1:N），找到对应节点邻居，获取此节点邻居标签，找到出现次数最大标签，若出现次数最多标签不止一个，则随机选择一个标签替换成此节点标签；  
3. 若节点标签不再变化，则迭代停止，否则重复第二步。  
### 相似度加权传播
每个节点标签按相似度传播给相邻节点，在节点传播的每一步，每个节点根据相邻节点的标签来更新自己的标签，与该节点相似度越大，其相邻节点对其标注的影响权值越大，相似节点的标签越趋于一致，其标签就越容易传播。在标签传播过程中，保持已标记的数据的标签不变，使其将标签传给未标注的数据。最终当迭代结束时，相似节点的概率分布趋于相似，可以划分到一类中。  
## 消息传递的思想
人类学习知识的过程。在自身具有一定知识的基础上，我们会想要从周围的伙伴那里学习到更多的知识，然后将伙伴给予的信息与自身已有的知识组合起来，更新并获得更高阶的知识。  
注意：该思想仅仅是一次消息传递过程的描述，并非能够生成很好的节点表征。  
未经过训练的图神经网络生成的节点表征还不是好的节点表征，好的节点表征可用于衡量节点之间的相似性。通过监督学习对图神经网络做很好的训练，图神经网络才可以生成好的节点表征。  
## 消息传递神经网络
做的事情与标签传播算法类似，只是消息传递神经网络不仅仅传递标签值，而是整个特征向量。  
具体算法步骤和卷积神经网络类似，先**聚合**信息然后**更新**一个新的表示作为未来的输入，如下图所示。   
<img src="F:\学习总结\Method-and-Technology\images\GCNN.jpg" alt="Graph Convolutional Network Model" style="zoom:50%;" />  
具体的定义和计算过程如下所示：  
$\mathbf{x}_i^{(k)} = \psi^{(k)} \left( \mathbf{x}_i^{(k-1)}, \rho_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right)$    
以上的消息传递过程包括三个主要函数  

1. 消息函数 φ：将边上特征与其两端节点的特征相结合来生成消息，一般使用线性转化实现。  
2. 聚合函数 ρ：聚合节点接受到的消息，函数必须是排列不变的，比如sum、max、mean等。  
3. 更新函数 ψ：结合聚合后的消息和节点本身的特征来更新节点的特征。
不同的消息传递神经网络会设计不同的三类函数，这会影响最终获得的表示向量。不同的任务会着重于抓取不同的信息来更好地完成目标。   
# PyTorch Geometric 消息传递源码分析
Pytorch Geometric（PyG）提供了 MessagePassing 基类，它封装了 ”消息传递” 的运行流程。通过继承 MessagePassing 基类，自定义 message() 方法（$\phi$）、update()方法（$\psi$），以及使用的消息聚合方案（$\rho$），就可以方便地构造消息传递图神经网络。   
## MessagePassing 基类
* MessagePassing(aggr="add", flow="source_to_target", node_dim=-2)：  
    * 对象初始化方法，设置聚合方案和消息传递的流向（边由 source 和 target 组成）；  
    * aggr：定义要使用的聚合方案（"add"、"mean "或 "max"）；  
    * flow：定义消息传递的流向（"source_to_target "或 "target_to_source"）；  
* MessagePassing.propagate(edge_index, size=None, \*\*kwargs)：  
    * 开始传递消息的起始调用，在此方法中message、update等方法被调用；  
    * 它以edge_index（边的端点的索引）和flow（消息流向）以及一些额外的数据为参数。  
    * propagate()不仅限于基于形状为[N, N]的对称邻接矩阵进行“消息传递过程”。基于非对称地邻接矩阵进行消息传递（当图为二部图时），需要传递参数size=(N, M)，否则认为是对称的。
    如果设置size=None，则认为邻接矩阵是对称的。  
* MessagePassing.message(...)；
    * 首先确定要给节点传递消息的边的集合；
    * 接着为各条边创建要传递给节点$i$的消息，即实现$\phi$函数；
    * MessagePassing.message(...)方法可以接收传递MessagePassing.propagate(edge_index, size=None, \*\*kwargs)方法的所有参数，我们在message()方法的参数列表里定义要接收的参数，例如我们要接收x,y,z参数，则我们应定义message(x,y,z)方法；
    * 传递给propagate()方法的参数，如果是节点的属性的话，可以被拆分成属于中心节点的部分和属于邻接节点的部分，只需在变量名后面加上\_i或\_j。
* MessagePassing.aggregate(...)：
    * 将从源节点传递过来的消息聚合在目标节点上，一般可选的聚合方式有sum, mean和max。
* MessagePassing.message_and_aggregate(...)：
    * 在一些场景里，邻接节点信息变换和邻接节点信息聚合这两项操作可以融合在一起，那么我们可以在此方法里定义这两项操作，从而让程序运行更加高效。
* MessagePassing.update(aggr_out, ...):
    * 为每个节点 $i \in \mathcal{V}$ 更新节点表征，即实现 $\gamma$ 函数。此方法以aggregate 方法的输出为第一个参数，并接收所有传递给 propagate() 方法的参数。
## MessagePassing 子类（GCNConv）
### 向量形式
$$ \mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right)$$   
其中，$\mathbf{\Theta}$ 是权重矩阵（即机器学习中要更新的参数），$\mathbf{x}_i^{(k)}$	表示节点 i 第 k 次迭代的特征向量，${deg(i)}$ 表示节点 i 的度，$\mathcal{N}(i)$ 表示节点 i 的所有邻居节点的集合。  
### GCNConv源码
```python
import torch
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add')  # "Add" aggregation.
        self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3-5: Start propagating messages.
        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)

    def message(self, x_j, edge_index, size):
        # x_j has shape [E, out_channels]
        # edge_index has shape [2, E]

        # Step 3: Normalize node features.
        row, col = edge_index
        deg = degree(row, size[0], dtype=x_j.dtype)  # [N, ]
        deg_inv_sqrt = deg.pow(-0.5)   # [N, ]
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        return norm.view(-1, 1) * x_j

    def update(self, aggr_out):
        # aggr_out has shape [N, out_channels]
        # Step 5: Return new node embeddings.
        return aggr_out
```
### 初始化init
这一部分主要是定义了一个线性变换的结构进行降维：
```python
def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add')  # "Add" aggregation.
        self.lin = torch.nn.Linear(in_channels, out_channels)
```
其中in_channels是节点特征的维度，out_channels是我们自己设定的降维维度。这里只是定义了结构，具体的逻辑实现是在forward()里实现的。这一部分对应着$\mathbf{X} \mathbf{\Theta}$。输入维度为(N, in_channels)，输出维度为(N, out_channels)。N是节点个数。  
<img src="F:\学习总结\Method-and-Technology\images\GCN_step1.png" alt="Graph Convolutional Network Model" style="zoom:100%;" />   

### forward
```python
def forward(self, x, edge_index):
    # x has shape [N, in_channels]
    # edge_index has shape [2, E]

    # Step 1: Add self-loops to the adjacency matrix.
    edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

    # Step 2: Linearly transform node feature matrix.
    x = self.lin(x)

    # Step 3-5: Start propagating messages.
    return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)
```
1、给邻接矩阵加上自循环，也即构造出矩阵 \mathbf{\hat{A}}   
但是如果用边的形式表示的话，相当于在原先的数组上加上source和target节点编号相同的边。例如，从[[0,1,1,2],[1,0,2,1]]变成了[[0,1,1,2,0,1,2],[1,0,2,1,0,1,2]]  
2、实现了线性变换：如在init里所说的一样。  
### message
```python
def message(self, x_j, edge_index, size):
    # x_j has shape [E, out_channels]
    # edge_index has shape [2, E]

    # Step 3: Normalize node features.
    row, col = edge_index
    deg = degree(row, size[0], dtype=x_j.dtype)  # [N, ]
    deg_inv_sqrt = deg.pow(-0.5)   # [N, ]
    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

    return norm.view(-1, 1) * x_j
```
3、对特征进行归一化
首先说明x_j的由来。这里E表示边的个数，对边矩阵edge_index，形状为(2, E)，第一行表示边的source节点（在代码中是row，这两者在本文中等价），第二行表示边的target节点（在代码中是col，这两者在本文中等价），如下示意图：  
<img src="F:\学习总结\Method-and-Technology\images\GCN_step2.png" alt="Graph Convolutional Network Model" style="zoom:100%;" />   
然后，以target节点作为索引，从线性变换后的特征矩阵中索引得到target节点的特征矩阵，示意图如下：   
<img src="F:\学习总结\Method-and-Technology\images\GCN_step3.png" alt="Graph Convolutional Network Model" style="zoom:80%;" />  
这就是x_j的由来，也是为什么形状为(E, out_channels)的原因。
在message函数中，首先计算了row(target)的度，这里默认图是无向图，row的度和col的度在结果上是一样的。deg[0]表示编号为0的节点的度，因此它的长度为N。而deg_inv_sqrt[row]返回了长度为E的度数组。例如，deg_inv_sqrt[0]表示第1条边的source的度的开根号，因此若把它与第一条边的target的度的开根号，就能得到标准化系数了。因此，norm最终保存了所有边的标准化系数。  
函数最后返回的是每一条边的标准化系数 × 这条边target这一端的节点特征。  
4、对邻居节点特征进行聚合操作  
按照source进行聚合，如下图所示：  
<img src="F:\学习总结\Method-and-Technology\images\GCN_step4.png" alt="Graph Convolutional Network Model" style="zoom:100%;" />  
这里有3条边的source都是节点0，因此将这三行向量聚合（相加sum，取均值mean，取最大值max都可以，这里用相加），最终得到一个形状为(N, out_channels)的特征矩阵。该矩阵，就是这一层GCN的输出。  

### update
```python
def update(self, aggr_out):
        # aggr_out has shape [N, out_channels]

        # Step 5: Return new node embeddings.
        return aggr_out
```
5、直接返回信息聚合的输出  
# 如何自定义消息传递图神经网络（GCN）
在继承 MessagePassing 基类的子类中重写 message(), aggreate(), message_and_aggreate() 和update() 即可自定义自己的消息传递图神经网络。  

因时间原因，具体内容之后会添加。  

# 参考资料
1. [标签传播算法（Label Propagation Algorithm, LPA）初探](https://www.cnblogs.com/LittleHann/p/10699988.html)
2. [DataWhale GNN 组会学习开源资料](https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/4-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md)
3. [Graph Neural Network • Introduction to Graph Neural Networks](https://perfectial.com/blog/graph-neural-networks-and-graph-convolutional-networks/)
4. [Torch geometric GCNConv 源码分析](https://blog.csdn.net/qq_41987033/article/details/103377561)