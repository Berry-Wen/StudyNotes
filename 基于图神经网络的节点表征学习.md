# 基于图神经网络的节点表征学习
## 引言
一个标准的机器学习流程如下：  
<img src="F:\学习总结\Method-and-Technology\images\机器学习流程.jpg" alt="机器学习流程" style="zoom:20%;" />  
因此，如何设计一套合理方式来高效地进行特征表示，是十分重要的。同样，在图节点预测或边预测任务中，我们首先需要生成节点表征（Node Representation）。  
本篇文章将以 Cora 数据集为例，讲解基于图神经网络的节点表征学习过程，比较 MLP 和 GCN，GAT 在节点分类任务中的性能。  
## 数据准备和简单分析
Cora 是一个论文引用网络数据集，节点代表论文，如果两篇论文存在引用关系，则对应的两个节点之间存在边，各节点的属性是一个1433维的词包特征向量。我们的任务是预测各篇论文的类别。  
### 在线下载 Cora 数据集
```python
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures
dataset = Planetoid(root='dataset', name='Cora', transform=NormalizeFeatures())
```
数据转换（transform）在数据输入到神经网络之前修改数据，可用于数据规范化或数据增强。相当于预处理环节，包括去除孤立点、增加自环边、行特征归一化等。在此例子中，我们使用NormalizeFeatures 进行节点特征归一化，使各节点特征总和为1。  
### 深入了解 Cora 数据集
查看 dataset 对象的各项属性：  
```python
print(f'Number of graphs: {len(dataset)}')
# Number of graphs: 1
print(f'Number of features: {dataset.num_features}')
# Number of features: 1433
print(f'Number of classes: {dataset.num_classes}')
# Number of classes: 7
```
选择第一个也是唯一一个图对象进行分析：  
```python
data = dataset[0]
print(f'Number of nodes: {data.num_nodes}')
# Number of nodes: 2708
print(f'Number of edges: {data.num_edges}') #无向边已经计算为2条
# Number of edges: 10556
print(f'Average node degree: {2 * data.num_edges / data.num_nodes:.2f}')
# Average node degree: 3.90
print(f'Number of training nodes: {data.train_mask.sum()}')
# Number of training nodes: 140
print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') # train_mask, test_mask等为True False向量
# Training node label rate: 0.05
print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')
# Contains isolated nodes: False
print(f'Contains self-loops: {data.contains_self_loops()}')
# Contains self-loops: False
print(f'Is undirected: {data.is_undirected()}')
# Is undirected: True
```
## 节点分类模型建立和评价
基于 Cora 数据集，我们可以构建论文的多分类模型。输出内容确立后，我们还需要确定输入内容和模型架构。最容易想到的是仅使用论文的 1433 个词包特征来构建线性分类模型。  
### MLP（多层感知器） 神经网络
MLP 是最基础的神经网络结构，它只对输入节点的特征做变换，在所有节点间共享权重。  
接下来将展示 MLP 的构造、训练和测试过程，这些过程与其他神经网络有很多共同之处。  
**MLP 神经网络的构造**  
```python
import torch # 深度学习库
from torch.nn import Linear # 最简单的神经网络架构
import torch.nn.functional as F # 包含架构设置中的诸多组件

class MLP(torch.nn.Module): 
    def __init__(self, hidden_channels): # 复写，设置一层隐藏层
        super(MLP, self).__init__()
        torch.manual_seed(12345) # 模型初始值随机，设置种子方便复现
        self.lin1 = Linear(dataset.num_features, hidden_channels)
        self.lin2 = Linear(hidden_channels, dataset.num_classes)

    def forward(self, x): # 前向传播过程，可设置激活函数、drop层等
        x = self.lin1(x)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin2(x)
        return x

model = MLP(hidden_channels=16)
print(model)
# MLP((lin1): Linear(in_features=1433, out_features=16, bias=True)
#     (lin2): Linear(in_features=16, out_features=7, bias=True))
```
其中，dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。  
Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如下图所示：  
<img src="F:\学习总结\Method-and-Technology\images\Dropout.png" alt="Dropout" style="zoom:80%;" />  
可参考：https://blog.csdn.net/program_developer/article/details/80737724  
另外，Relu是非线性激活函数，相比Sigmoid函数，Relu的输出不存在梯度消失的问题，且计算简单速度快。可参考博客：https://zhuanlan.zhihu.com/p/46255482  
**MLP 神经网络的训练**  
神经网络的训练方法是大致相同的，分类模型流行使用交叉熵损失函数和Adam优化器。  
```python
criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.

def train():
    model.train()
    optimizer.zero_grad()  # Clear gradients.
    out = model(data.x)  # Perform a single forward pass.
    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.
    loss.backward()  # Derive gradients.
    optimizer.step()  # Update parameters based on gradients.
    return loss

for epoch in range(1, 201):
    loss = train()
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')
# Epoch: 001, Loss: 1.9615
# Epoch: 002, Loss: 1.9557
# Epoch: 003, Loss: 1.9505
# Epoch: ......
# Epoch: 198, Loss: 0.4664
# Epoch: 199, Loss: 0.3714
# Epoch: 200, Loss: 0.3810
```
Adam Optimizer是对SGD的扩展，可以代替经典的随机梯度下降法来更有效地更新网络权重。  
optimizer.step() 这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。不需要显式地将loss和optimizer联系起来（Python中的一种语法，让用起来更方便）。model 和 model.forward 是一个意思。  
交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。  
**MLP 神经网络的测试**  
训练完模型后，我们可以通过测试来检验这个简单的MLP神经网络在测试集上的表现。  

```python
def test():
    model.eval()
    out = model(data.x)
    pred = out.argmax(dim=1)  # Use the class with highest probability.
    test_correct = pred[data.test_mask] == data.y[data.test_mask] # 使用 test_mask 来区分训练集、测试集
    test_acc = int(test_correct.sum()) / int(data.test_mask.sum()) 
    return test_acc

test_acc = test()
print(f'Test Accuracy: {test_acc:.4f}')
# Test Accuracy: 0.5900
```
**为什么MLP没有表现得更好呢？**其中一个重要原因是，用于训练此神经网络的有标签节点数量过少，此神经网络被过拟合，它对未见过的节点泛化能力很差。训练集只占 5%。  
### GCN 图神经网络
可以考虑论文之间的引用关系， 详细内容请大家参阅 [GCNConv 官方文档](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv)。  
**GCN 图神经网络的构造**  
将上面例子中的 torch.nn.Linear 替换成torch_geometric.nn.GCNConv，我们就可以得到一个GCN图神经网络：  
```python
from torch_geometric.nn import GCNConv # 与上一个例子的不同之处
class GCN(torch.nn.Module):
    def __init__(self, hidden_channels): # 与上一个例子类似，设置各层神经元数
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GCNConv(dataset.num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)

    def forward(self, x, edge_index): # 与上一个例子类似，但输入了边数据
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x

model = GCN(hidden_channels=16)
print(model)
# GCN((conv1): GCNConv(1433, 16)
#     (conv2): GCNConv(16, 7))
```
**GCN 图神经网络的训练和测试与 MLP 一致，不再赘述**  
实验结果 Test Accuracy: 0.8140，比 MLP 要好很多，说明了考虑节点关系的重要性。  
### GAT 图注意力神经网络
将注意力机制加入到了图神经网络中。可以将MLP神经网络例子中的 torch.nn.Linear 替换成torch_geometric.nn.GATConv 来实现，如下方代码所示：  
```python
from torch_geometric.nn import GATConv # 与MLP的不同之处
class GAT(torch.nn.Module): # 与MLP类似，设置各层神经元数量
    def __init__(self, hidden_channels):
        super(GAT, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GATConv(dataset.num_features, hidden_channels)
        self.conv2 = GATConv(hidden_channels, dataset.num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x
```
GATConv构造函数接口：  
```python
GATConv(in_channels: Union[int, Tuple[int, int]], out_channels: int, heads: int = 1, concat: bool = True, negative_slope: float = 0.2, dropout: float = 0.0, add_self_loops: bool = True, bias: bool = True, **kwargs) 
```
详细内容请大家参阅 [GATConv官方文档](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv)  
与MLP、GCN的不同之处在于，构造模型时需要使用 heads 参数来选择使用多少个注意力模型，以及设置 concat 参数选择是否拼接不同注意力模型得到的节点表征。  
**GAT 图注意力神经网络的训练和测试与 MLP 一致，不再赘述**  
经过测试，当使用默认参数时，Test Accuracy: 0.7380  
## 模型在其他数据集的表现
上文发现，GCN 在 Cora 数据集上有最好的节点分类性能，但是也不能说 GCN 优于其他模型。这需要在多个数据集上进行实验才可。本节将上文中的模型重新运行在 CiteSeer 数据集上进行实验。  
### 在线下载 CiteSeer 数据集
```python
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures
dataset = Planetoid(root='dataset', name='CiteSeer', transform=NormalizeFeatures())
```
### 深入了解 CiteSeer 数据集
查看 dataset 对象的各项属性：  
```python
print(f'Number of graphs: {len(dataset)}')
# Number of graphs: 1
print(f'Number of features: {dataset.num_features}')
# Number of features: 3703
print(f'Number of classes: {dataset.num_classes}')
# Number of classes: 6
```
可以看到，相比 Cora 数据集，节点的特征数量增加到了2倍。  
选择第一个也是唯一一个图对象进行分析：  
```python
data = dataset[0]
print(f'Number of nodes: {data.num_nodes}')
# Number of nodes: 3327
print(f'Number of edges: {data.num_edges}') #无向边已经计算为2条
# Number of edges: 9104
print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')
# Average node degree: 2.74
print(f'Number of training nodes: {data.train_mask.sum()}')
# Number of training nodes: 120
print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') # train_mask, test_mask等为True False向量
# Training node label rate: 0.04
print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')
# Contains isolated nodes: True
print(f'Contains self-loops: {data.contains_self_loops()}')
# Contains self-loops: False
print(f'Is undirected: {data.is_undirected()}')
# Is undirected: True
```
### 节点分类模型建立与比较
模型构造、训练和测试与上文相同，不再赘述（均设置一层隐藏层，神经元数目为16）。  
**MLP** Test Accuracy: 0.5820  
**GCN** Test Accuracy: 0.7120  
**GAT** Test Accuracy: 0.6100  
可以发现，在 CiteSeer 数据集中，GCN 仍然表现最优，但相比在 Cora 数据集上的性能还是差了不少，其他模型也有相应的性能降低。  
## 遗留问题，有待思考解决  
* 为什么 Cora 的训练节点只有 5%，传统的机器学习方法不都是 80% 吗？  
* 如何设置 GAT 的 head 等参数呢，好像需要自定义 forward 函数？  
* 目前还不是很能够自由构造模型，需要进一步学习有关Python类以及神经网络的知识  
## 参考资料
1. [Datawhale 组队学习 GNN 教程](https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/5-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md)
2. [知乎文章 图节点表征学习](https://zhuanlan.zhihu.com/p/306261981)